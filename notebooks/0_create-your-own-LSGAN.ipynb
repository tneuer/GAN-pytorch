{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db7e4650",
   "metadata": {},
   "source": [
    "# Creating your own GAN I\n",
    "\n",
    "In this notebook you will learn how to create your own Generative Adversarial Network with `vegans`. This is a more advanced topic which gives you deeper insights into the design of the library. As the time of writing this notebook (2021-04-08 08:32) there are only 3 (6) GAN architectures implemented in `vegans`: `VanillaGAN`, `WasssersteinGAN`, `WassersteinGANGP` and all there conditional variants. In this notebook I will explain to you how to implement the `LSGAN` and `ConditionalLSGAN` (which will then probably be part of the library after finishing this notebook). In the next few tutorials we will create successively implement more difficult architectures (Pix2Pix, LR-GAN).\n",
    "\n",
    "First import the usual libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b545fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "os.chdir(\"/home/thomas/Backup/Algorithmen/GAN-pytorch\")\n",
    "from vegans.GAN import ConditionalWassersteinGAN, ConditionalWassersteinGANGP\n",
    "from vegans.utils.utils import plot_losses, plot_images, get_input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978cd6f1",
   "metadata": {},
   "source": [
    "## GenerativeModel\n",
    "\n",
    "We will first implement the unconditional variant of the `LSGAN`, investigate the base classes to be used and the move on to the conditional version.\n",
    "\n",
    "The most important class in `vegans` is the `GenerativeModel`. It takes care of a lot of boilerplate code for logging and saving stuff, checking for the correct input and defining the correct variables. Every unconditional model (and also conditional models for that matter) should inherit from this class. It is semi-abstract in the sense that the `GenerativeModel` itself can not be used for training anything as it's missing some important functions which MUST be implemented by its children. \n",
    "\n",
    "These **abstract methods** are\n",
    "- __init__(self, x_dim, z_dim, optim, optim_kwargs, fixed_noise_size, device, folder, ngpu):\n",
    "    This takes care of the initializaton and the method \n",
    "    \n",
    "    GenerativeModel.__init__(\n",
    "        self, x_dim=x_dim, z_dim=z_dim, optim=optim, optim_kwargs=optim_kwargs,\n",
    "        fixed_noise_size=fixed_noise_size, device=device, folder=folder, ngpu=ngpu\n",
    "     )\n",
    "     \n",
    "     must be called at the end of the `__init__` method.\n",
    "- _default_optimizer(self): returns an optimizer from torch.optim that is used if the user doesn't specify any optimizers in the `optim` keyword when constructing a class.\n",
    "- _define_loss(self): Not strictly necessary but it is still kept as an abstract method so that the user has to think about what he wants to implement here. You can also implement it with a single `pass` statement. However, we will show you it's intended use here.\n",
    "- calculate_losses(self, X_batch, Z_batch, who): The core function that needs to be implemented. For every batch it must populate an already existing (but empty) dictionary `self._losses`. The keys of the dictionary must include at least the keys of the used `self.neural_nets`, but can also contain other losses. We will discuss this more when we come to it.\n",
    "\n",
    "The `GenerativeModel` will also check for the presence of one very **important** attribute:\n",
    "- self.neural_nets: This is a dictionary containing all the different networks to be trained. It might look like\n",
    "    {\n",
    "        \"Generator\": generator_nn_Module,\n",
    "        \"Adversariat\": adversariat_nn_Module,\n",
    "        \"Encoder\": encoder_nn_Module\n",
    "    }\n",
    "\n",
    "The values of the dictionary must inherit in one way or another from `nn.Module`. The user of the implemented GAN must make sure of that by using `nn.Sequential` or building their own architectures which inherit from `nn.Module` (like shown in all previous tutorials).\n",
    "\n",
    "The keys of the dictionary are equally as important because these will link together different parts used during training:\n",
    "- self.optimizers: dict containing the same keys as `self.neural_nets`. Containing one optimizer per network.\n",
    "- self.steps: dict containing the same keys as `self.neural_nets`. Containing the number of training steps per network.\n",
    "- self._losses: dict containing the same keys as `self.neural_nets`. Containing the loss functions per network.\n",
    "\n",
    "Which key will be used per training step is determined by the `who`argument of `calculate_losses`. In this example case `who` will be one of \"Generator\", \"Adversariat\" or \"Encoder\". All of these will be called in succession.\n",
    "\n",
    "Now we covered most of the important things. They will be explained again at the appropriate position over the course of the next few notebooks whenever they become relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba67f14",
   "metadata": {},
   "source": [
    "## GAN1v1\n",
    "\n",
    "We can almost start with the implementation of the LSGAN. There exists one more utility class which is not as abstract as `GenerativeModel` (it in fact inherits from `GenerativeModel`) but not yet a true `GAN` implementation. This is the `GAN1v1` which should be used whenever you want to implement a `GAN` of the structure \n",
    "\n",
    "self.neural_nets = {\n",
    "    \"Generator\": generator_nn_Module,\n",
    "    \"Adversariat\": adversariat_nn_Module\n",
    "}\n",
    "\n",
    "So one generator vs one adversariat. This includes the VanillaGAN, WassersteinGAN, WassersteinGANGP as well as the LSGAN. It already implements the `calculate_losses` abstract method (which can be overriden of course) and takes care of initialization. So implementing LSGAN becomes very easy. More advanced cases are in the next notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d309299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vegans.models.unconditional.GAN1v1 import GAN1v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c19eaee",
   "metadata": {},
   "source": [
    "## LSGAN\n",
    "\n",
    "Now let's with the class definition and `__init__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9717130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSGAN(GAN1v1):\n",
    "    def __init__(\n",
    "            self,\n",
    "            generator,\n",
    "            adversariat,\n",
    "            x_dim,\n",
    "            z_dim,\n",
    "            optim=None,\n",
    "            optim_kwargs=None,\n",
    "            fixed_noise_size=32,\n",
    "            device=None,\n",
    "            folder=\"./LSGAN\",\n",
    "            ngpu=None):\n",
    "\n",
    "        super().__init__(\n",
    "            generator=generator, adversariat=adversariat,\n",
    "            z_dim=z_dim, x_dim=x_dim, adv_type=\"Discriminator\",\n",
    "            optim=optim, optim_kwargs=optim_kwargs,\n",
    "            fixed_noise_size=fixed_noise_size,\n",
    "            device=device, folder=folder, ngpu=ngpu\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c27afa",
   "metadata": {},
   "source": [
    "This is basically a copy of the code for the [VanillaGAN](https://github.com/tneuer/GAN-pytorch/blob/main/vegans/models/unconditional/VanillaGAN.py). We do not need to inherit from `GenerativeModel` explicitly because this is already done by `GAN1v1`.\n",
    "\n",
    "As for all networks we expect an optim(izer) dictionary, optim_kwargs (optimizer keyword arguments), fixed_noise_size (for logging purposes), the device (\"cpu\" or \"cuda\"), folder and ngpu (number gpus). We simply pass this to the parent class [GAN1v1](https://github.com/tneuer/GAN-pytorch/blob/main/vegans/models/unconditional/GAN1v1.py) which will immediatly create the very important\n",
    "\n",
    "`self.neural_nets = {\"Generator\": self.generator, \"Adversariat\": self.adversariat}`\n",
    "\n",
    "You are bound by these names (\"Generator\", \"Adversariat\") if you are using `GAN1v1`. If you don't like them you need to implement a little bit more (next notebooks).\n",
    "\n",
    "Notice that we used `adv_type=\"Discriminator\"` which indicates that the `adversariat` must output a value between [0, 1]. This will be checked when the user passes an adversariat architecture. If you want the output to be between [-Inf, Inf] use `adv_type=\"Critic\"`.\n",
    "\n",
    "Because they are so simple we will implement the two missing methods for `_default_optimizer` and `_define_loss` in one go. The loss function must take two arguments:\n",
    "\n",
    "- output from discriminator (or critic).\n",
    "- real and false labels. They will be generated by the `GAN1v1` and are either arrays full of ones or zeros.\n",
    "\n",
    "It is stated in the paper approximately as:\n",
    "\n",
    "Discriminator: 0.5 \\* ( (D(x) - b)\\*\\*2 (D(G(z)) - a)\\*\\*2 )\n",
    "\n",
    "Generator: 0.5 \\*  (D(G(z)) - c)\\*\\*2 \n",
    "\n",
    "where D(x) is the discriminator output (predictions), G(z) is the generator output and a, b, c are parameters. Very often we set a=0, b=c=1. This is what we will do in our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca40f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSGAN(GAN1v1):\n",
    "    def __init__(\n",
    "            self,\n",
    "            generator,\n",
    "            adversariat,\n",
    "            x_dim,\n",
    "            z_dim,\n",
    "            optim=None,\n",
    "            optim_kwargs=None,\n",
    "            fixed_noise_size=32,\n",
    "            device=None,\n",
    "            folder=\"./VanillaGAN\",\n",
    "            ngpu=None):\n",
    "\n",
    "        super().__init__(\n",
    "            generator=generator, adversariat=adversariat,\n",
    "            z_dim=z_dim, x_dim=x_dim, adv_type=\"Discriminator\",\n",
    "            optim=optim, optim_kwargs=optim_kwargs,\n",
    "            fixed_noise_size=fixed_noise_size,\n",
    "            device=device, folder=folder, ngpu=ngpu\n",
    "        )\n",
    "        \n",
    "    def _default_optimizer(self):\n",
    "        return torch.optim.Adam\n",
    "\n",
    "    def _define_loss(self):\n",
    "        self.loss_functions = {\"Generator\": torch.nn.MSELoss(), \"Adversariat\": torch.nn.MSELoss()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d23a4",
   "metadata": {},
   "source": [
    "We chose the `torch.optim.Adam` optimizer as a default and implemented the appropriate loss. The parent classes `GAN1v1` and `GenerativeModel` will take care of all the rest. \n",
    "\n",
    "The network would now be ready to be used :)\n",
    "\n",
    "But we won't stop here and go quickly over the implementation of the `ConditionalLSGAN` so we can take labels and images as conditional input.\n",
    "\n",
    "## ConditionalLSGAN\n",
    "\n",
    "We can basically do the same thing as before and copy from [CondtionalVanillaGAN](https://github.com/tneuer/GAN-pytorch/blob/main/vegans/models/conditional/ConditionalVanillaGAN.py). This time we will inherit from `ConditionalGAN1v1` (which inherits from `ConditionalGenerativeModel` which in turn inherits from `GenerativeModel`). Everything is a `GenerativeModel` in the end. \n",
    "\n",
    "The main difference is that we now must also pass the `y_dim` (Dimension of the labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "004e96ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vegans.models.conditional.ConditionalGAN1v1 import ConditionalGAN1v1\n",
    "\n",
    "class ConditionalLSGAN(ConditionalGAN1v1):\n",
    "    def __init__(\n",
    "            self,\n",
    "            generator,\n",
    "            adversariat,\n",
    "            x_dim,\n",
    "            z_dim,\n",
    "            y_dim,\n",
    "            optim=None,\n",
    "            optim_kwargs=None,\n",
    "            fixed_noise_size=32,\n",
    "            device=None,\n",
    "            folder=\"./ConditionalVanillaGAN\",\n",
    "            ngpu=None):\n",
    "\n",
    "        super().__init__(\n",
    "            generator=generator, adversariat=adversariat,\n",
    "            x_dim=x_dim, z_dim=z_dim, y_dim=y_dim, adv_type=\"Discriminator\",\n",
    "            optim=optim, optim_kwargs=optim_kwargs,\n",
    "            fixed_noise_size=fixed_noise_size,\n",
    "            device=device, folder=folder, ngpu=ngpu\n",
    "        )\n",
    "\n",
    "    def _default_optimizer(self):\n",
    "        return torch.optim.Adam\n",
    "\n",
    "    def _define_loss(self):\n",
    "        self.loss_functions = {\"Generator\": torch.nn.MSELoss(), \"Adversariat\": torch.nn.MSELoss()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c092f7",
   "metadata": {},
   "source": [
    "We did not need to change much at all. This algorithm should now be possible to generate specific instances of handwritten digits or even translate an image of a summer scenery into winter scenery (note that there are other special architectures for especially this last problem, like CycleGAN)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
